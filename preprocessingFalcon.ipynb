{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/home/nogaschw/Codeworkout/cleaned_code.csv'\n",
    "cwd = os.getcwd()\n",
    "\n",
    "df = pd.read_csv(csv_file_path, sep=',')\n",
    "df = df[df.columns[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_file_path = '/home/nogaschw/Codeworkout/cleaned_questions.csv'\n",
    "\n",
    "questions = pd.read_csv(cleaned_file_path, sep=',')\n",
    "questions = questions[questions.columns[1:]]\n",
    "questions = questions[questions['max_score'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['problem_id'] = df['problem_id'].apply(lambda x: x.lower())\n",
    "questions['id'] = questions['id'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_895104/2430072954.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(['student_id', 'problem_id', 'course_id']).apply(lambda x: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values('timestamp', ascending=True)\n",
    "df = df.groupby(['student_id', 'problem_id', 'course_id']).apply(lambda x: pd.Series({\n",
    "    'ServerTimestamp': x['timestamp'].tolist(),\n",
    "    'score': x['score'].tolist(),\n",
    "    'source_code': x['clean_code'].tolist()\n",
    "})).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_after_max(score, snapshots, ServerTimestamp):\n",
    "    max_index = len(score) - 1 - score[::-1].index(max(score))\n",
    "    return score[:max_index+1], snapshots[:max_index+1], ServerTimestamp[:max_index+1]\n",
    "\n",
    "# Apply the function to each row\n",
    "df['score'], df['source_code'], df['ServerTimestamp'] = zip(*df.apply(lambda row: trim_after_max(row['score'], row['source_code'], row['ServerTimestamp']), axis=1))\n",
    "df['struggling'] = df['score'].apply(lambda x: 0 if x[-1] == 100 else 1)\n",
    "df['start_time'] = [i[0] for i in df['ServerTimestamp']]\n",
    "df['max_score'] = [max(i) for i in df['score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['max_score'] == -1])\n",
    "df = df[df['max_score'] != -1] # remove all the rows contain only locall code (not really the grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "struggling\n",
       "0    115052\n",
       "1     27659\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['struggling'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2404043389076244"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "27659 / 115052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic = df.merge(questions, how='inner', left_on=['problem_id', 'course_id'], right_on=['id', 'course_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic = df_basic[['student_id', 'problem_id', 'course_id', 'start_time', 'score', 'source_code', 'prompt', 'struggling']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df_basic.sort_values(by=['student_id', 'start_time'])\n",
    "df_sorted['row_number'] = df_sorted.groupby('student_id').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_code = df_sorted['source_code'].tolist()\n",
    "prev_code.insert(0, [])\n",
    "prev_q = df_sorted['prompt'].tolist()\n",
    "prev_q.insert(0, [])\n",
    "prev_num_q = df_sorted['problem_id'].tolist()\n",
    "prev_num_q.insert(0, [])\n",
    "prev_score = df_sorted['score'].tolist()\n",
    "prev_score.insert(0, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted['prev_code'] = prev_code[:-1] # last question of the last student\n",
    "df_sorted['prev_question'] = prev_q[:-1]\n",
    "df_sorted['prev_question_num'] = prev_num_q[:-1]\n",
    "df_sorted['score'] = prev_score[:-1]\n",
    "\n",
    "df_sorted = df_sorted[df_sorted['row_number'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_code = []\n",
    "prev_question = []\n",
    "prev_q_num = []\n",
    "prev_score = []\n",
    "for i, row in df_sorted.iterrows():\n",
    "    if row['row_number'] == 2:\n",
    "        prev_code.append([row['prev_code']])\n",
    "        prev_question.append([row['prev_question']])\n",
    "        prev_q_num.append([row['prev_question_num']])\n",
    "        prev_score.append([row['score']])\n",
    "    else:\n",
    "        prev_code.append(prev_code[-1] + [row['prev_code']])\n",
    "        prev_question.append(prev_question[-1] + [row['prev_question']])\n",
    "        prev_q_num.append(prev_q_num[-1] + [row['prev_question_num']])\n",
    "        prev_score.append(prev_score[-1] + [row['score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted['prev_code'] = prev_code\n",
    "df_sorted['prev_question'] = prev_question\n",
    "df_sorted['prev_question_num'] = prev_q_num\n",
    "df_sorted['score'] = prev_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_with_compu = questions.copy()\n",
    "q_with_compu['IfElse'] = questions['conditional']\n",
    "q_with_compu['Loops'] = questions[[\"loop_counting\", \"loop_until\", \"loop_elements\", \"loop_nested\"]].max(axis=1)\n",
    "q_with_compu['MathOperations'] = questions[[\"stat_calculate\", \"assignment\"]].max(axis=1)\n",
    "q_with_compu['LogicOperators'] = 0\n",
    "q_with_compu['StringOperations'] = questions[[\"input_str\", \"input_cast\", \"output\"]].max(axis=1)\n",
    "q_with_compu[\"List\"]= questions[[\"list\", \"list_2d\"]].max(axis=1)\n",
    "q_with_compu['FileOperations'] = questions[[\"file_read\", \"file_write\"]].max(axis=1)\n",
    "q_with_compu[\"Functions\"] = questions[[\"function_call\", \"function_def\", \"function_return\"]].max(axis=1)\n",
    "q_with_compu['Dictionary'] = questions[\"dictionary\"]\n",
    "q_with_compu['Tuple'] = questions[[\"tuple\", \"item_set\"]].max(axis=1)\n",
    "\n",
    "q_with_compu = q_with_compu.drop(columns=['input_str', 'input_cast', 'output', 'assignment', 'conditional',\n",
    "       'function_call', 'function_def', 'function_return', 'loop_counting',\n",
    "       'loop_until', 'loop_elements', 'loop_nested', 'stat_calculate',\n",
    "       'file_read', 'file_write', 'list', 'list_2d', 'dictionary', 'item_set',\n",
    "       'tuple'])\n",
    "\n",
    "\n",
    "q_with_compu.fillna(0, inplace=True)\n",
    "questions.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_with_compu = q_with_compu.rename(columns={'prompt': 'question'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxonomy = df_sorted.merge(q_with_compu, how='inner', left_on=['problem_id', 'course_id'], right_on=['id', 'course_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxonomy = df_taxonomy[['student_id', 'course_id', 'prev_code', 'prev_question', 'question', 'struggling', 'type', 'IfElse', 'Loops',\n",
    "       'MathOperations', 'LogicOperators', 'StringOperations', 'List', 'FileOperations', 'Functions', 'Dictionary', 'Tuple']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/nogaschw/Codeworkout/Thesis/Data/falcon/split_ids_valid_ids.pkl', 'rb') as f:\n",
    "        valid_ids = pickle.load(f)\n",
    "with open(f'/home/nogaschw/Codeworkout/Thesis/Data/falcon/split_ids_test_ids.pkl', 'rb') as f:\n",
    "    test_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxonomy = df_taxonomy[df_taxonomy['type'] != 'project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df_taxonomy[df_taxonomy['student_id'].isin(valid_ids)]\n",
    "test_df = df_taxonomy[df_taxonomy['student_id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([valid_df, test_df], ignore_index=True).to_pickle(os.path.join(cwd, '../df_taxonomy_falcon_test_valid.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_snapshots = 0\n",
    "max_total = 0\n",
    "max_row_snapshots = None\n",
    "max_row_total = None\n",
    "for j, row in df_taxonomy.iterrows():\n",
    "    r = row['prev_code']\n",
    "    sum_snapshots = 0\n",
    "    for i in r:\n",
    "        sum_snapshots += len(i)\n",
    "        if len(i) > max_snapshots:\n",
    "            max_snapshots = len(i)\n",
    "            max_row_snapshots = row\n",
    "    if sum_snapshots > max_total:\n",
    "        max_total = sum_snapshots\n",
    "        max_row_total = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2109"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nogaschw/.conda/envs/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Data import *\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "lr = 0.00001\n",
    "all_questions = 1\n",
    "taxonomy = 0\n",
    "balance_def = 1\n",
    "mean = 1\n",
    "\n",
    "balance_def = [oversampling_boosting, oversampling_df, undersampling_df, oversampling_augmentation][balance_def]\n",
    "\n",
    "name = f'{[\"\", \"Mean_\"][mean]}{\"All_History_with\"}_{[\"Current_Question\", \"All_Question\", \"FullyConnected\"][all_questions]}_{hidden_size}_{num_layers}_{lr}_{balance_def.__name__}'\n",
    "\n",
    "dataset = [DatasetCodeQuestion, DatasetCodeQPrevQ, DatasetLimitMean][all_questions]\n",
    "\n",
    "feature = None\n",
    "\n",
    "text_model_name = 'google-bert/bert-base-uncased'\n",
    "code_model_name = 'microsoft/codebert-base'\n",
    "\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(code_model_name)\n",
    "if text_tokenizer.pad_token is None:\n",
    "    text_tokenizer.pad_token = text_tokenizer.eos_token\n",
    "if code_tokenizer.pad_token is None:\n",
    "    code_tokenizer.pad_token = code_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load exist spliting\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = create_data_loader(df_taxonomy, DatasetCodeQuestion, text_tokenizer, code_tokenizer, batch_size=32, balanced_def=balance_def, feature=feature, ids_filepath_prefix=\"/home/nogaschw/Codeworkout/Thesis/Data/falcon/split_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# Check data leak\n",
    "print(set(train_dataloader.dataset.df['student_id']).intersection(set(test_dataloader.dataset.df['student_id'])))\n",
    "print(set(valid_dataloader.dataset.df['student_id']).intersection(set(test_dataloader.dataset.df['student_id'])))\n",
    "print(set(valid_dataloader.dataset.df['student_id']).intersection(set(train_dataloader.dataset.df['student_id'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
