{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nogaschw/.conda/envs/clean_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import from unsloth\n",
    "from unsloth import FastModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_data_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 1. Load the Fine-Tuned Model\n",
    "#########################\n",
    "# IMPORTANT: Make sure you pass the local path you used to save your model\n",
    "# e.g. new_model_local = \"Gemma-3-12B-it-FirstResponder\"\n",
    "FINETUNED_MODEL_PATH = \"google/gemma-3-12b-it\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Loading the fine-tuned model...\")\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = FINETUNED_MODEL_PATH,\n",
    "    max_seq_length = 1024,\n",
    "    load_in_4bit = True,\n",
    "    load_in_8bit = False,\n",
    "    full_finetuning = False\n",
    "    # token = \"hf_...\"  # If your model is gated and requires a token\n",
    ")\n",
    "\n",
    "# # Because we used the gemma-3 chat template, fetch it again\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma-3\",  # same template used during finetuning\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the QWAN model...\n",
      "==((====))==  Unsloth 2025.4.7: Fast Qwen2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.643 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################\n",
    "# 1. Load the QWAN Model\n",
    "#########################\n",
    "# IMPORTANT: Replace with your actual model path if using a local fine-tuned version\n",
    "FINETUNED_MODEL_PATH = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Loading the QWAN model...\")\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = FINETUNED_MODEL_PATH,\n",
    "    max_seq_length = 1024,\n",
    "    load_in_4bit = True,\n",
    "    load_in_8bit = False,\n",
    "    full_finetuning = False\n",
    "    # token = \"hf_...\"  # If gated\n",
    ")\n",
    "\n",
    "# # Use the Qwen chat template\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template=\"qwen\",  # match your finetuning template\n",
    "# )\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Config import Config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 2. Load the Test Data\n",
    "########################\n",
    "data = 0\n",
    "snapshot = -1\n",
    "name = f\"{'CW' if data == 0 else 'FC'}sn{'first' if snapshot == 0 else 'last'}\"\n",
    "\n",
    "path_df = config.path_saved_codeworkout if data == 0 else config.path_saved_falcon\n",
    "questions_df = pd.read_excel(config.codeworkout_questions_path) if data == 0 else pd.read_csv(config.falconcode_questions_path)\n",
    "id_question = 'ProblemID' if data == 0 else 'id'\n",
    "prompt = 'Requirement' if data == 0 else 'prompt'\n",
    "\n",
    "df = pd.read_pickle(path_df)\n",
    "df = df[['student_id', 'prev_tasks_id', 'prev_tasks']] if data == 0 else df[['student_id', 'course_id', 'prev_tasks_id', 'prev_tasks']]\n",
    "df = df[~df['student_id'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are a computer science teacher evaluating student code. \"\n",
    "    \"Evaluate the following programming skills on a scale from 1 to 5, where \"\n",
    "    \"1 = weak, 2 = fair, 3 = average, 4 = strong, 5 = excellent. \"\n",
    "    \"Provide ONLY a JSON object, no extra text, no explanation, no code block, no markdown.Format strictly like this:\\n\"\n",
    "    \"{\\\"Decomposition\\\": 3, \\\"Algorithmic Design\\\": 4, \\\"Reading Comprehension\\\": 5}\"\n",
    "    \"Definitions:\\n\"\n",
    "    \"- Decomposition: breaking the problem into parts â€” 1 = no understanding, 2 = only simple parts, \"\n",
    "    \"3 = basic decomposition, 4 = mostly complete, 5 = excellent decomposition.\\n\"\n",
    "    \"- Algorithmic Design: choosing and structuring algorithms â€” 1 = no algorithm or wrong approach, \"\n",
    "    \"2 = basic approach, 3 = reasonable structure, 4 = good design, 5 = excellent design.\\n\"\n",
    "    \"- Reading Comprehension: understanding the problem description â€” 1 = misunderstood task, \"\n",
    "    \"2 = partially understood, 3 = mostly understood, 4 = good understanding, 5 = excellent understanding.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 4. Helper: Build Chat Template\n",
    "#########################\n",
    "def build_prompt(user_input):\n",
    "    \"\"\"\n",
    "    Build the conversation with roles: system, user.\n",
    "    We'll pass that to tokenizer.apply_chat_template(...).\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": user_input}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    # Return the formatted text ready for generation\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,               # â† important!\n",
    "        add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_input):\n",
    "    \"\"\"\n",
    "    Build prompt for Qwen1.5-Chat using role-based messages.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data == 0:\n",
    "    df['user_input'] = df.apply(\n",
    "        lambda row: [\n",
    "            f'Task: {questions_df[questions_df[id_question] == int(row[\"prev_tasks_id\"][i])][prompt].iloc[0]}\\n\\n'\n",
    "            f'Studentâ€™s code:\\n{row[\"prev_tasks\"][i][snapshot]}'\n",
    "            for i in range(len(row['prev_tasks_id']))\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "else:\n",
    "    # clean question\n",
    "    questions_df['prompt'] = questions_df['prompt'].apply(lambda x: x.split('PROBLEM STATEMENT:')[-1] if x.__contains__(\"PROBLEM STATEMENT:\") else x)\n",
    "    questions_df['prompt'] = questions_df.prompt.apply(lambda text: re.sub(r'\\bPROBLEM STATEMENT: \\b', '', text).strip())\n",
    "    questions_df[id_question] = questions_df[id_question].apply(lambda x: x.lower())\n",
    "    df['user_input'] = df.apply(\n",
    "        lambda row: [\n",
    "            f\"Task: {questions_df[(questions_df[id_question] == row['prev_tasks_id'][i]) & (questions_df['course_id'] == row['course_id'])][prompt].iloc[0]}\\n\\n\"\n",
    "            f\"Studentâ€™s code:\\n{row['prev_tasks'][i][snapshot]}\"\n",
    "            for i in range(len(row['prev_tasks_id']))\n",
    "        ],\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(batch_prompts):\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True)\n",
    "    prompt_lens = [len(input_ids) for input_ids in inputs['input_ids']]\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            temperature=0.3,\n",
    "            top_p=0.95,\n",
    "            top_k=64,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    batch_results = []\n",
    "    for j in range(outputs.shape[0]):\n",
    "        gen_text = tokenizer.decode(outputs[j][prompt_lens[j]:], skip_special_tokens=True)\n",
    "        batch_results.append(gen_text)\n",
    "        \n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(batch_prompts):\n",
    "    tokenized = [tokenizer(p, return_tensors=\"pt\") for p in batch_prompts]\n",
    "    prompt_lens = [t[\"input_ids\"].shape[-1] for t in tokenized]\n",
    "\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            temperature=0.3,\n",
    "            top_p=0.95,\n",
    "            top_k=64,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    results = []\n",
    "    for out, prompt_len in zip(outputs, prompt_lens):\n",
    "        text = tokenizer.decode(out[prompt_len:], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        results.append(text)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = [build_prompt(prompt) for row in df['user_input'].head(2) for prompt in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:36<00:00,  4.51s/it]\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# 5. Generate Predictions\n",
    "#########################\n",
    "batch_size = 8\n",
    "all_outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_prompts), batch_size)):\n",
    "    batch = all_prompts[i:i + batch_size]\n",
    "    outputs = gen(batch)  # your existing function\n",
    "    all_outputs.extend(outputs)\n",
    "    if (i // batch_size) % 50 == 0 and i != 0:\n",
    "        print(f\"{i} / {len(all_prompts)}\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_scores(text, i, loop_num=0):\n",
    "    # Find the first JSON block in the text\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text).lower()  # removes non-ASCII\n",
    "    match = re.search(r'\\{[\\s\\S]*?\\}', text)\n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            decomposition = data.get(\"decomposition\")\n",
    "            algorithmic = data.get(\"algorithmic_design\") or data.get(\"algorithmic design\")\n",
    "            reading = data.get(\"reading comprehension\") or data.get(\"reading_comprehension\")\n",
    "            return decomposition, algorithmic, reading\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to parse JSON!\")\n",
    "            if loop_num > 2:\n",
    "                return None, None, None\n",
    "            return extract_scores(gen(all_prompts[i])[0], i, loop_num+1)\n",
    "    else:\n",
    "        print(\"No JSON found!\")\n",
    "        if loop_num > 2:\n",
    "            return None, None, None\n",
    "        return extract_scores(gen(all_prompts[i])[0], i, loop_num+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON found!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "No JSON found!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "No JSON found!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "No JSON found!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "No JSON found!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n",
      "Failed to parse JSON!\n"
     ]
    }
   ],
   "source": [
    "decomposition = []\n",
    "alg = []\n",
    "reading = []\n",
    "for i, o in enumerate(all_outputs): \n",
    "    decomp, algo, read = extract_scores(o, i)\n",
    "    decomposition.append(decomp)\n",
    "    alg.append(algo)\n",
    "    reading.append(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' JSON:\\n\\n```java\\n{\\n  \"decomposition\": {\\n    \"decomposition_level\": 4,\\n    \"algorithmic_design\": 4,\\n    \"reading_comprehension\": 4\\n  },\\n  \"feedback\": {\\n    \"strengths\": [\\n      \"Decomposition is well-structured and organized.\",\\n      \"Algorithmic design'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^\\x00-\\x7F]+', '', all_outputs[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ç†Ÿè¯»å¹¶è¯„ä¼°ä»¥ä¸‹ä»£ç ï¼Œç„¶åŽç»™å‡ºç›¸åº”çš„ JSON å¯¹è±¡è¡¨ç¤º\\n\\n```json\\n{\\n  \"decomposition\": 4,\\n  \"algorithmic_design\": 4,\\n  \"reading_comprehension\": 5\\n}\\n```\\n\\nThe given code snippet is a simple Java function named `answerCell` that takes three boolean parameters: `isMorning`, `isMom`, and `is'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (2) does not match length of index (630)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     a.append(alg[i:i+\u001b[32m30\u001b[39m])\n\u001b[32m      7\u001b[39m     r.append(reading[i:i+\u001b[32m30\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdecomposition\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = d\n\u001b[32m     10\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33malg\u001b[39m\u001b[33m'\u001b[39m] = a\n\u001b[32m     11\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mreading\u001b[39m\u001b[33m'\u001b[39m] = r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/clean_env/lib/python3.11/site-packages/pandas/core/frame.py:4311\u001b[39m, in \u001b[36mDataFrame.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4308\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_array([key], value)\n\u001b[32m   4309\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4310\u001b[39m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4311\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/clean_env/lib/python3.11/site-packages/pandas/core/frame.py:4524\u001b[39m, in \u001b[36mDataFrame._set_item\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4514\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4515\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4516\u001b[39m \u001b[33;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[32m   4517\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4522\u001b[39m \u001b[33;03m    ensure homogeneity.\u001b[39;00m\n\u001b[32m   4523\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4524\u001b[39m     value, refs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4526\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4527\u001b[39m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns\n\u001b[32m   4528\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m value.ndim == \u001b[32m1\u001b[39m\n\u001b[32m   4529\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value.dtype, ExtensionDtype)\n\u001b[32m   4530\u001b[39m     ):\n\u001b[32m   4531\u001b[39m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[32m   4532\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.is_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/clean_env/lib/python3.11/site-packages/pandas/core/frame.py:5266\u001b[39m, in \u001b[36mDataFrame._sanitize_column\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   5263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m.index)\n\u001b[32m   5265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[32m-> \u001b[39m\u001b[32m5266\u001b[39m     \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5267\u001b[39m arr = sanitize_array(value, \u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   5268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   5269\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[32m   5270\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m value.dtype == \u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5273\u001b[39m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[32m   5274\u001b[39m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/clean_env/lib/python3.11/site-packages/pandas/core/common.py:573\u001b[39m, in \u001b[36mrequire_length_match\u001b[39m\u001b[34m(data, index)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) != \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLength of values \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdoes not match length of index \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Length of values (2) does not match length of index (630)"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "a = []\n",
    "r = []\n",
    "for i in range(0, len(decomposition), 30):\n",
    "    d.append(decomposition[i:i+30])\n",
    "    a.append(alg[i:i+30])\n",
    "    r.append(reading[i:i+30])\n",
    "\n",
    "df['decomposition'] = d\n",
    "df['alg'] = a\n",
    "df['reading'] = r\n",
    "\n",
    "df.to_csv(r'/home/nogaschw/Codeworkout/Thesis/gemma1to5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
