{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nogaschw/.conda/envs/clean_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "sys.path.append('../..')\n",
    "from Model.helper import *\n",
    "from Config import Config\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../Data'))\n",
    "from Data import *\n",
    "from choosedataset import *\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 False similarity: False\n",
      "[100.0, 0.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 0.0, 100.0, 100.0, 100.0, 100.0, 100.0, 0.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "data = [Codeworkout, Falcon][1]()\n",
    "df = data.df\n",
    "padding_size_code = 30\n",
    "loss_func = False\n",
    "df['num_snapshots'] = df['prev_tasks'].apply(lambda x: [len(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_future_q = set()\n",
    "for i in df['new_task_id']:\n",
    "    all_future_q.add(i)\n",
    "\n",
    "all_prev_q = set()\n",
    "for i in df['prev_tasks_id']:\n",
    "    all_prev_q = all_prev_q.union(set(i))\n",
    "all_problems = all_future_q.union(all_prev_q)\n",
    "vocab = {name: idx for idx, name in enumerate(all_problems)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAKTDatasetFromDataFrame(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, max_len_code=768, padding_size_code=100, padding_size_q=30):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.seq_len = padding_size_code\n",
    "        self.vocab = text_tokenizer\n",
    "        self.question_num = len(text_tokenizer)\n",
    "        print(self.question_num)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        # Extract relevant data\n",
    "        prev_q = row['prev_tasks_id']\n",
    "        prev_a = row['prev_labels']\n",
    "        target_q = row['new_task_id']\n",
    "        label = row['Label']\n",
    "\n",
    "        # Convert to SAKT input format (q_id if correct, q_id + QUESTION_NUM if incorrect)\n",
    "        input_seq = []\n",
    "        for q, a in zip(prev_q, prev_a):\n",
    "            q = self.vocab[q]\n",
    "            a = bool(a)\n",
    "            input_seq.append(q if a else q + self.question_num)\n",
    "\n",
    "        # Padding/truncation\n",
    "        if len(input_seq) >= self.seq_len:\n",
    "            input_seq = input_seq[-self.seq_len:]\n",
    "        else:\n",
    "            input_seq = [0] * (self.seq_len - len(input_seq)) + input_seq\n",
    "\n",
    "        return {\n",
    "            'label': torch.tensor([int(label)]).long(),\n",
    "            'input': torch.tensor(input_seq).long(),\n",
    "            'target_id': torch.tensor(self.vocab[target_q]).long()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAKTDatasetAttemptFromDataFrame(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, max_len_code=768, padding_size_code=100, padding_size_q=30):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.seq_len = padding_size_code\n",
    "        self.vocab = text_tokenizer\n",
    "        self.question_num = len(text_tokenizer)\n",
    "        print(self.question_num)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        # Extract relevant data\n",
    "        prev_q = row['prev_tasks_id']\n",
    "        prev_a = row['prev_labels']\n",
    "        target_q = row['new_task_id']\n",
    "        label = row['Label']\n",
    "        num_snapshots = row['num_snapshots']\n",
    "\n",
    "        # Convert to SAKT input format (q_id if correct, q_id + QUESTION_NUM if incorrect)\n",
    "        input_seq = []\n",
    "        for i, (q, a) in enumerate(zip(prev_q, prev_a)):\n",
    "            q = self.vocab[q]\n",
    "            a = bool(a)\n",
    "            for j in range(num_snapshots[i] - 1):\n",
    "                input_seq.append(q + self.question_num)\n",
    "            input_seq.append(q if a else q + self.question_num)\n",
    "\n",
    "        # Padding/truncation\n",
    "        if len(input_seq) >= self.seq_len:\n",
    "            input_seq = input_seq[-self.seq_len:]\n",
    "        else:\n",
    "            input_seq = [0] * (self.seq_len - len(input_seq)) + input_seq\n",
    "\n",
    "        return {\n",
    "            'label': torch.tensor([int(label)]).long(),\n",
    "            'input': torch.tensor(input_seq).long(),\n",
    "            'target_id': torch.tensor(self.vocab[target_q]).long()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Based on Annotated Transformer from Harvard NLP:\n",
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html#applications-of-attention-in-our-model\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model, bias=False), 4) # Q, K, V, last\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask,\n",
    "                                 dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "class SAKTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Encoder block of SAKT\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_head, dropout):\n",
    "        super().__init__()\n",
    "        self._self_attn = MultiHeadedAttention(num_head, hidden_dim, dropout)\n",
    "        self._ffn = PositionwiseFeedForward(hidden_dim, hidden_dim, dropout)\n",
    "        self._layernorms = clones(nn.LayerNorm(hidden_dim, eps=1e-6), 2)\n",
    "\n",
    "    def forward(self, query, key, mask=None):\n",
    "        \"\"\"\n",
    "        query: question embeddings\n",
    "        key: interaction embeddings\n",
    "        \"\"\"\n",
    "        # self-attention block\n",
    "        output = self._self_attn(query=query, key=key, value=key, mask=mask)\n",
    "        output = self._layernorms[0](key + output)\n",
    "        # feed-forward block\n",
    "        output = self._layernorms[1](output + self._ffn(output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class SAKT(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based\n",
    "    all hidden dimensions (d_k, d_v, ...) are the same as hidden_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, question_num, num_layers, num_head, dropout):\n",
    "        super().__init__()\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._question_num = question_num\n",
    "\n",
    "        # Blocks\n",
    "        self._layers = clones(SAKTLayer(hidden_dim, num_head, dropout), num_layers)\n",
    "\n",
    "        # prediction layer\n",
    "        self._prediction = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Embedding layers\n",
    "        self._positional_embedding = nn.Embedding(padding_size_code+1, hidden_dim, padding_idx=0)\n",
    "        self._interaction_embedding = nn.Embedding(2*question_num+1, hidden_dim, padding_idx=0)\n",
    "        self._question_embedding = nn.Embedding(question_num+1, hidden_dim, padding_idx=0)\n",
    "\n",
    "    def _transform_interaction_to_question_id(self, interaction):\n",
    "        \"\"\"\n",
    "        get question_id from interaction index\n",
    "        if interaction index is a number in [0, question_num], then leave it as-is\n",
    "        if interaction index is bigger than question_num (in [question_num + 1, 2 * question_num]\n",
    "        then subtract question_num\n",
    "        interaction: integer tensor of shape (batch_size, sequence_size)\n",
    "        \"\"\"\n",
    "        return interaction - self._question_num * (interaction > self._question_num).long()\n",
    "\n",
    "    def _get_position_index(self, question_id):\n",
    "        \"\"\"\n",
    "        [0, 0, 0, 4, 12] -> [0, 0, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        batch_size = question_id.shape[0]\n",
    "        position_indices = []\n",
    "        for i in range(batch_size):\n",
    "            non_padding_num = (question_id[i] != 0).sum(-1).item()\n",
    "            position_index = [0] * (padding_size_code - non_padding_num) + list(range(1, non_padding_num+1))\n",
    "            position_indices.append(position_index)\n",
    "        return torch.tensor(position_indices, dtype=int).to(question_id.device)\n",
    "\n",
    "    def forward(self, interaction_id, target_id):\n",
    "        \"\"\"\n",
    "        Query: Question (skill, exercise, ...) embedding\n",
    "        Key, Value: Interaction embedding + positional embedding\n",
    "        \"\"\"\n",
    "        question_id = self._transform_interaction_to_question_id(interaction_id)\n",
    "        question_id = torch.cat([question_id[:, 1:], target_id.unsqueeze(1)], dim=-1)\n",
    "        interaction_vector = self._interaction_embedding(interaction_id)\n",
    "        question_vector = self._question_embedding(question_id)\n",
    "        position_index = self._get_position_index(question_id)\n",
    "        position_vector = self._positional_embedding(position_index)\n",
    "\n",
    "        mask = get_pad_mask(question_id, 0) & get_subsequent_mask(question_id)\n",
    "        x = interaction_vector + position_vector\n",
    "\n",
    "        for layer in self._layers:\n",
    "            x = layer(query=question_vector, key=x, mask=mask)\n",
    "\n",
    "        output = self._prediction(x)\n",
    "        output = output[:, -1, :]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKT(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM based model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, question_num, dropout):\n",
    "        super().__init__()\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._num_layers = num_layers\n",
    "        self._lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self._encoder = nn.Embedding(num_embeddings=2*question_num+1, embedding_dim=input_dim, padding_idx=0)\n",
    "        self._decoder = nn.Linear(hidden_dim, question_num)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        initialize hidden layer as zero tensor\n",
    "        batch_size: single integer\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self._num_layers, batch_size, self._hidden_dim),\n",
    "                weight.new_zeros(self._num_layers, batch_size, self._hidden_dim))\n",
    "\n",
    "    def forward(self, input, target_id):\n",
    "        \"\"\"\n",
    "        get model output (before taking sigmoid) for target_id\n",
    "        input: (batch_size, sequence_size)\n",
    "        target_id: (batch_size)\n",
    "        return output, a tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        input = self._encoder(input)\n",
    "        output, _ = self._lstm(input, (hidden[0].detach(), hidden[1].detach()))\n",
    "        output = self._decoder(output[:, -1, :])\n",
    "        output = torch.gather(output, -1, target_id.unsqueeze(1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_1loss(batch, model, device, criterion, eval=False):\n",
    "    dict_batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    model_params = {k: v for k, v in dict_batch.items() if k != 'label'}\n",
    "    logits = model(*model_params.values())\n",
    "    label = dict_batch['label'].float()\n",
    "    if not criterion:\n",
    "        return logits, label\n",
    "    return criterion(logits, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAKT(100, len(vocab), 1, 5, 0.2)\n",
    "# model = DKT(100, 100, 1, len(vocab), 0.2)\n",
    "caculate_func = caculate_1loss\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=1e-4)\n",
    "\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test, val, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing splitting\n",
      "408\n",
      "408\n",
      "408\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = create_data_loader(df, SAKTDatasetFromDataFrame, padding_size_code=padding_size_code, \n",
    "                                                                         text_tokenizer=vocab, batch_size=config.batch_size\n",
    "                                                                         , create_split=False, ids_filepath_prefix='/home/nogaschw/Codeworkout/UseData/falcon/split_ids_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559 81 164\n",
      "Label\n",
      "False    10813\n",
      "True      7053\n",
      "Name: count, dtype: int64\n",
      "Label\n",
      "False    1511\n",
      "True     1054\n",
      "Name: count, dtype: int64\n",
      "Label\n",
      "False    3244\n",
      "True     2003\n",
      "Name: count, dtype: int64\n",
      "930 132 267\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader), len(valid_dataloader), len(test_dataloader), flush=True)\n",
    "print(train_dataloader.dataset.df['Label'].value_counts())\n",
    "print(valid_dataloader.dataset.df['Label'].value_counts())\n",
    "print(test_dataloader.dataset.df['Label'].value_counts())\n",
    "print(len(set(train_dataloader.dataset.df['student_id'])), len(set(valid_dataloader.dataset.df['student_id'])), len(set(test_dataloader.dataset.df['student_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/04/2025_20:10:20\n",
      "559 81\n",
      "Epoch: 0\n",
      "Batch 0 from 559\n",
      "Batch 100 from 559\n",
      "Batch 200 from 559\n",
      "Batch 300 from 559\n",
      "Batch 400 from 559\n",
      "Batch 500 from 559\n",
      "Test Batch 0 from 81\n",
      "Epoch [1], LR: 0.000100, Loss: 0.5842, Val Loss: 0.6021, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 1\n",
      "Batch 0 from 559\n",
      "Batch 100 from 559\n",
      "Batch 200 from 559\n",
      "Batch 300 from 559\n",
      "Batch 400 from 559\n",
      "Batch 500 from 559\n",
      "Test Batch 0 from 81\n",
      "Epoch [2], LR: 0.000100, Loss: 0.5777, Val Loss: 0.6017, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 2\n",
      "Batch 0 from 559\n",
      "Batch 100 from 559\n",
      "Batch 200 from 559\n",
      "Batch 300 from 559\n",
      "Batch 400 from 559\n",
      "Batch 500 from 559\n",
      "Test Batch 0 from 81\n",
      "Epoch [3], LR: 0.000100, Loss: 0.5728, Val Loss: 0.6028, patience: 5\n",
      "Epoch: 3\n",
      "Batch 0 from 559\n",
      "Batch 100 from 559\n",
      "Batch 200 from 559\n",
      "Batch 300 from 559\n",
      "Batch 400 from 559\n",
      "Batch 500 from 559\n",
      "Test Batch 0 from 81\n",
      "Epoch [4], LR: 0.000100, Loss: 0.5672, Val Loss: 0.6053, patience: 4\n",
      "Epoch: 4\n",
      "Batch 0 from 559\n",
      "Batch 100 from 559\n",
      "Batch 200 from 559\n",
      "Batch 300 from 559\n",
      "Batch 400 from 559\n",
      "Batch 500 from 559\n",
      "Test Batch 0 from 81\n",
      "Epoch [5], LR: 0.000100, Loss: 0.5619, Val Loss: 0.6073, patience: 3\n",
      "Epoch: 5\n",
      "Batch 0 from 559\n",
      "Batch 100 from 559\n",
      "Batch 200 from 559\n",
      "Batch 300 from 559\n",
      "Batch 400 from 559\n",
      "Batch 500 from 559\n",
      "Test Batch 0 from 81\n",
      "Epoch [6], LR: 0.000100, Loss: 0.5569, Val Loss: 0.6063, patience: 2\n",
      "Epoch: 6\n",
      "Batch 0 from 559\n",
      "Batch 100 from 559\n",
      "Batch 200 from 559\n",
      "Batch 300 from 559\n",
      "Batch 400 from 559\n",
      "Batch 500 from 559\n",
      "Test Batch 0 from 81\n",
      "Epoch [7], LR: 0.000100, Loss: 0.5517, Val Loss: 0.6075, patience: 1\n",
      "18/04/2025_20:12:04\n",
      "Loaded best model weights.\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model = training_loop(model=model, train_dataloader=train_dataloader, test_dataloader=valid_dataloader, \n",
    "                      optimizer=optimizer, criterion=criterion, device=device, name='a', caculate_func=caculate_func, use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(threshold, y_true, y_prob):\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.where(y_prob > threshold, 1, 0)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    best = \"best\"\n",
    "    if threshold == 0.5:\n",
    "        best = \"0.5\"\n",
    "    #  df = pd.concat([pd.DataFrame([[model_name, threshold, roc_auc, accuracy, precision, recall, f1]], columns=df.columns), df], ignore_index=True)\n",
    "    print({\"threshold\": threshold, \"roc_auc\": roc_auc, \"accuracy\": accuracy, f\"precision_{best}\": precision, f\"recall_{best}\": recall, f\"f1_{best}\": f1})\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch 0 from 81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch 0 from 164\n",
      "Test Batch 100 from 164\n",
      "{'threshold': 0.5, 'roc_auc': np.float64(0.7367446056562506), 'accuracy': 0.7032590051457976, 'precision_0.5': np.float64(0.6519073569482289), 'recall_0.5': np.float64(0.4777833250124813), 'f1_0.5': np.float64(0.5514261019878998)}\n",
      "[[2733  511]\n",
      " [1046  957]]\n"
     ]
    }
   ],
   "source": [
    "all_labels, all_probs = eval_loop(model, valid_dataloader, device, caculate_func=caculate_func)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "J = tpr - fpr\n",
    "best_index = J.argmax()\n",
    "\n",
    "y_labels, y_probs = eval_loop(model, test_dataloader, device, caculate_func=caculate_func)\n",
    "results(0.5, y_labels, y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_probs > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    'y_true': y_labels.squeeze(1),\n",
    "    'y_probs': y_probs.squeeze(1),\n",
    "    'y_pred': y_pred.squeeze(1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5247"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([2733 , 511,1046 , 957])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2384, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n",
      "408\n",
      "408\n",
      "408\n",
      "408\n",
      "408\n",
      "408\n",
      "408\n",
      "408\n",
      "408\n"
     ]
    }
   ],
   "source": [
    "data_loaders = create_data_loader_k_fold(df, SAKTDatasetFromDataFrame, vocab, batch_size=config.batch_size, padding_size_code=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642 162\n",
      "1064 266\n",
      "set()\n",
      "Label\n",
      "False    0.610281\n",
      "True     0.389719\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.591271\n",
      "True     0.408729\n",
      "Name: proportion, dtype: float64\n",
      "642 162\n",
      "1064 266\n",
      "set()\n",
      "Label\n",
      "False    0.607456\n",
      "True     0.392544\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.602549\n",
      "True     0.397451\n",
      "Name: proportion, dtype: float64\n",
      "642 162\n",
      "1064 266\n",
      "set()\n",
      "Label\n",
      "False    0.6049\n",
      "True     0.3951\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.612691\n",
      "True     0.387309\n",
      "Name: proportion, dtype: float64\n",
      "643 162\n",
      "1064 266\n",
      "set()\n",
      "Label\n",
      "False    0.606084\n",
      "True     0.393916\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.607995\n",
      "True     0.392005\n",
      "Name: proportion, dtype: float64\n",
      "646 158\n",
      "1064 266\n",
      "set()\n",
      "Label\n",
      "False    0.603631\n",
      "True     0.396369\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.618084\n",
      "True     0.381916\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def num_of(train_dataloader, test_dataloader):\n",
    "    print(len(train_dataloader), len(test_dataloader))\n",
    "    print(len(set(train_dataloader.dataset.df['student_id'])), len(set(test_dataloader.dataset.df['student_id'])))\n",
    "    print(set(train_dataloader.dataset.df['student_id']).intersection(set(test_dataloader.dataset.df['student_id'])))\n",
    "    print(train_dataloader.dataset.df.Label.value_counts(normalize=True))\n",
    "    print(test_dataloader.dataset.df.Label.value_counts(normalize=True))\n",
    "\n",
    "for train, test in data_loaders:\n",
    "    num_of(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "SAKT(\n",
      "  (_layers): ModuleList(\n",
      "    (0): SAKTLayer(\n",
      "      (_self_attn): MultiHeadedAttention(\n",
      "        (linears): ModuleList(\n",
      "          (0-3): 4 x Linear(in_features=100, out_features=100, bias=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_ffn): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (w_2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_layernorms): ModuleList(\n",
      "        (0-1): 2 x LayerNorm((100,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_prediction): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (_positional_embedding): Embedding(31, 100, padding_idx=0)\n",
      "  (_interaction_embedding): Embedding(817, 100, padding_idx=0)\n",
      "  (_question_embedding): Embedding(409, 100, padding_idx=0)\n",
      ")\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Fold 1, Epoch 0: Loss = 0.6749125838836777\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Fold 1, Epoch 10: Loss = 0.5604912662338988\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Test Batch 0 from 162\n",
      "Test Batch 100 from 162\n",
      "Fold 2:\n",
      "SAKT(\n",
      "  (_layers): ModuleList(\n",
      "    (0): SAKTLayer(\n",
      "      (_self_attn): MultiHeadedAttention(\n",
      "        (linears): ModuleList(\n",
      "          (0-3): 4 x Linear(in_features=100, out_features=100, bias=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_ffn): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (w_2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_layernorms): ModuleList(\n",
      "        (0-1): 2 x LayerNorm((100,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_prediction): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (_positional_embedding): Embedding(31, 100, padding_idx=0)\n",
      "  (_interaction_embedding): Embedding(817, 100, padding_idx=0)\n",
      "  (_question_embedding): Embedding(409, 100, padding_idx=0)\n",
      ")\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Fold 2, Epoch 0: Loss = 0.6725467926709452\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Fold 2, Epoch 10: Loss = 0.5614501566530388\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Test Batch 0 from 162\n",
      "Test Batch 100 from 162\n",
      "Fold 3:\n",
      "SAKT(\n",
      "  (_layers): ModuleList(\n",
      "    (0): SAKTLayer(\n",
      "      (_self_attn): MultiHeadedAttention(\n",
      "        (linears): ModuleList(\n",
      "          (0-3): 4 x Linear(in_features=100, out_features=100, bias=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_ffn): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (w_2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_layernorms): ModuleList(\n",
      "        (0-1): 2 x LayerNorm((100,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_prediction): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (_positional_embedding): Embedding(31, 100, padding_idx=0)\n",
      "  (_interaction_embedding): Embedding(817, 100, padding_idx=0)\n",
      "  (_question_embedding): Embedding(409, 100, padding_idx=0)\n",
      ")\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Fold 3, Epoch 0: Loss = 0.6754631362043065\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Fold 3, Epoch 10: Loss = 0.5650356264779129\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Batch 0 from 642\n",
      "Batch 100 from 642\n",
      "Batch 200 from 642\n",
      "Batch 300 from 642\n",
      "Batch 400 from 642\n",
      "Batch 500 from 642\n",
      "Batch 600 from 642\n",
      "Test Batch 0 from 162\n",
      "Test Batch 100 from 162\n",
      "Fold 4:\n",
      "SAKT(\n",
      "  (_layers): ModuleList(\n",
      "    (0): SAKTLayer(\n",
      "      (_self_attn): MultiHeadedAttention(\n",
      "        (linears): ModuleList(\n",
      "          (0-3): 4 x Linear(in_features=100, out_features=100, bias=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_ffn): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (w_2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_layernorms): ModuleList(\n",
      "        (0-1): 2 x LayerNorm((100,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_prediction): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (_positional_embedding): Embedding(31, 100, padding_idx=0)\n",
      "  (_interaction_embedding): Embedding(817, 100, padding_idx=0)\n",
      "  (_question_embedding): Embedding(409, 100, padding_idx=0)\n",
      ")\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Fold 4, Epoch 0: Loss = 0.67887536849983\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Fold 4, Epoch 10: Loss = 0.5637465763611148\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Batch 0 from 643\n",
      "Batch 100 from 643\n",
      "Batch 200 from 643\n",
      "Batch 300 from 643\n",
      "Batch 400 from 643\n",
      "Batch 500 from 643\n",
      "Batch 600 from 643\n",
      "Test Batch 0 from 162\n",
      "Test Batch 100 from 162\n",
      "Fold 5:\n",
      "SAKT(\n",
      "  (_layers): ModuleList(\n",
      "    (0): SAKTLayer(\n",
      "      (_self_attn): MultiHeadedAttention(\n",
      "        (linears): ModuleList(\n",
      "          (0-3): 4 x Linear(in_features=100, out_features=100, bias=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_ffn): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (w_2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (_layernorms): ModuleList(\n",
      "        (0-1): 2 x LayerNorm((100,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_prediction): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (_positional_embedding): Embedding(31, 100, padding_idx=0)\n",
      "  (_interaction_embedding): Embedding(817, 100, padding_idx=0)\n",
      "  (_question_embedding): Embedding(409, 100, padding_idx=0)\n",
      ")\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Fold 5, Epoch 0: Loss = 0.6786479846611849\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Fold 5, Epoch 10: Loss = 0.5620413675396804\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Batch 0 from 646\n",
      "Batch 100 from 646\n",
      "Batch 200 from 646\n",
      "Batch 300 from 646\n",
      "Batch 400 from 646\n",
      "Batch 500 from 646\n",
      "Batch 600 from 646\n",
      "Test Batch 0 from 158\n",
      "Test Batch 100 from 158\n"
     ]
    }
   ],
   "source": [
    "fold_results = {'ROC-AUC' : [], 'f1' : [], 'recall': [], \"precision\": [], 'calibration': [], 'f1-0.5': [], 'recall-0.5': [], 'precision-0.5': []}\n",
    "\n",
    "for fold, (train_dataloader, test_dataloader) in enumerate(data_loaders):\n",
    "    print(f\"Fold {fold + 1}:\")    # Prepare data for current fold\n",
    "    m = SAKT(100, len(vocab), 1, 5, 0.2)\n",
    "    loss_fn = None\n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=config.lr, weight_decay=1e-4)\n",
    "\n",
    "    m = m.to(device)\n",
    "    print(m)\n",
    "    # Training Loop\n",
    "    for epoch in range(config.epoch):\n",
    "        total_loss = train_loop(m, train_dataloader, device, optimizer, criterion, caculate_func)\n",
    "\n",
    "        # Optional: Print metrics every few epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Fold {fold + 1}, Epoch {epoch}: Loss = {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "    y_labels, y_probs = eval_loop(m, test_dataloader, device, caculate_func=caculate_func)\n",
    "    y_prob = np.array(y_probs)\n",
    "    y_true = np.array(y_labels)\n",
    "    y_pred = np.where(y_prob > 0.4, 1, 0)\n",
    "\n",
    "    fold_results['ROC-AUC'].append(roc_auc_score(y_true, y_prob))\n",
    "    fold_results['calibration'].append(brier_score_loss(y_true, y_prob))\n",
    "    fold_results['precision'].append(precision_score(y_true, y_pred))\n",
    "    fold_results['recall'].append(recall_score(y_true, y_pred))\n",
    "    fold_results['f1'].append(f1_score(y_true, y_pred))\n",
    "\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    fold_results['precision-0.5'].append(precision_score(y_true, y_pred))\n",
    "    fold_results['recall-0.5'].append(recall_score(y_true, y_pred))\n",
    "    fold_results['f1-0.5'].append(f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fold Results:\n",
      "ROC-AUC: 0.7078\n",
      "f1: 0.5816\n",
      "recall: 0.6056\n",
      "precision: 0.5605\n",
      "calibration: 0.2082\n",
      "f1-0.5: 0.5317\n",
      "recall-0.5: 0.4657\n",
      "precision-0.5: 0.6214\n"
     ]
    }
   ],
   "source": [
    "avg_results = {metric: np.mean(vals) for metric, vals in fold_results.items()}\n",
    "print(\"Average Fold Results:\")\n",
    "for metric, avg in avg_results.items():\n",
    "    print(f\"{metric}: {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROC-AUC': [np.float64(0.7150437953976683),\n",
       "  np.float64(0.7129646501457727),\n",
       "  np.float64(0.697864655079685),\n",
       "  np.float64(0.708057629893216),\n",
       "  np.float64(0.7049015365108301)],\n",
       " 'f1': [0.5946960807322225,\n",
       "  0.5815230365242301,\n",
       "  0.5612648221343873,\n",
       "  0.5874970950499652,\n",
       "  0.582881395897194],\n",
       " 'recall': [0.6013289036544851,\n",
       "  0.5918367346938775,\n",
       "  0.5674325674325674,\n",
       "  0.6257425742574257,\n",
       "  0.6417445482866043],\n",
       " 'precision': [0.5882079851439183,\n",
       "  0.5715626466447677,\n",
       "  0.555229716520039,\n",
       "  0.5536574682435392,\n",
       "  0.5339092872570195],\n",
       " 'calibration': [np.float64(0.20792951829272677),\n",
       "  np.float64(0.20687835195458407),\n",
       "  np.float64(0.2086698211655877),\n",
       "  np.float64(0.20839414344421903),\n",
       "  np.float64(0.20937398016348213)],\n",
       " 'f1-0.5': [0.5377410468319559,\n",
       "  0.5332955511476339,\n",
       "  0.5070671378091873,\n",
       "  0.5348314606741573,\n",
       "  0.5455561766349916],\n",
       " 'recall-0.5': [0.4632178452776459,\n",
       "  0.45724003887269193,\n",
       "  0.43006993006993005,\n",
       "  0.47128712871287126,\n",
       "  0.5067497403946002],\n",
       " 'precision-0.5': [0.6408404464871963,\n",
       "  0.6397008837525493,\n",
       "  0.6176470588235294,\n",
       "  0.6181818181818182,\n",
       "  0.5907990314769975]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CW paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fold Results:\n",
      "ROC-AUC: 0.7620\n",
      "f1: 0.5263\n",
      "recall: 0.6500\n",
      "precision: 0.4431\n",
      "calibration: 0.1529\n",
      "f1-0.5: 0.4214\n",
      "recall-0.5: 0.3332\n",
      "precision-0.5: 0.5778\n"
     ]
    }
   ],
   "source": [
    "avg_results = {metric: np.mean(vals) for metric, vals in fold_results.items()}\n",
    "print(\"Average Fold Results:\")\n",
    "for metric, avg in avg_results.items():\n",
    "    print(f\"{metric}: {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROC-AUC': [np.float64(0.7740033688938798),\n",
       "  np.float64(0.766852629855958),\n",
       "  np.float64(0.7290721755962397),\n",
       "  np.float64(0.7838333531999034),\n",
       "  np.float64(0.7561909398720871)],\n",
       " 'f1': [0.5734177215189873,\n",
       "  0.5206489675516224,\n",
       "  0.5187032418952618,\n",
       "  0.5317750182615048,\n",
       "  0.48689771766694845],\n",
       " 'recall': [0.7167721518987342,\n",
       "  0.6383363471971067,\n",
       "  0.6274509803921569,\n",
       "  0.7040618955512572,\n",
       "  0.5636007827788649],\n",
       " 'precision': [0.4778481012658228,\n",
       "  0.4396014943960149,\n",
       "  0.44208289054197664,\n",
       "  0.4272300469483568,\n",
       "  0.42857142857142855],\n",
       " 'calibration': [np.float64(0.15695245200834182),\n",
       "  np.float64(0.14797644107157734),\n",
       "  np.float64(0.17756763596985),\n",
       "  np.float64(0.13690343877733022),\n",
       "  np.float64(0.14516276947394102)],\n",
       " 'f1-0.5': [0.47092469018112487,\n",
       "  0.43132803632236094,\n",
       "  0.3800813008130081,\n",
       "  0.431980906921241,\n",
       "  0.392811296534018],\n",
       " 'recall-0.5': [0.39082278481012656,\n",
       "  0.3435804701627486,\n",
       "  0.28205128205128205,\n",
       "  0.35009671179883944,\n",
       "  0.299412915851272],\n",
       " 'precision-0.5': [0.592326139088729,\n",
       "  0.5792682926829268,\n",
       "  0.5825545171339563,\n",
       "  0.5638629283489096,\n",
       "  0.5708955223880597]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7611767027823713)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.float64(0.7687714553763709),\n",
    "  np.float64(0.7627186350314897),\n",
    "  np.float64(0.7411323079772276),\n",
    "  np.float64(0.7726746847475827),\n",
    "  np.float64(0.7605864307791853)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7442102629167655)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.float64(0.7535154834078422),\n",
    "  np.float64(0.7587025316455696),\n",
    "  np.float64(0.7116689015084738),\n",
    "  np.float64(0.7471109165634106),\n",
    "  np.float64(0.7500534814585316)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7658729791915045)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.float64(0.7649885571326023),\n",
    "  np.float64(0.7639131383675252),\n",
    "  np.float64(0.7478029991398976),\n",
    "  np.float64(0.7896520879431288),\n",
    "  np.float64(0.7630081133743692)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7414668086001812)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.float64(0.7495451282525106),\n",
    "  np.float64(0.7461553750701503),\n",
    "  np.float64(0.7324244383067913),\n",
    "  np.float64(0.7507600215159963),\n",
    "  np.float64(0.7284490798554578)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7052446566747911)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.float64(0.7097928165956),\n",
    "np.float64(0.698991739552964),\n",
    "np.float64(0.7030397767688579),\n",
    "np.float64(0.7021694955962242),\n",
    "np.float64(0.7122294548603094)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6966476822725202)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.float64(0.6943674191888805),\n",
    "  np.float64(0.6967422316912113),\n",
    "  np.float64(0.6914856535949052),\n",
    "  np.float64(0.7075981329380943),\n",
    "  np.float64(0.6930449739495101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7029110249755174)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.float64(0.7123435255808972),\n",
    "  np.float64(0.7005605080860182),\n",
    "  np.float64(0.6949550291830051),\n",
    "  np.float64(0.7020821153293113),\n",
    "  np.float64(0.7046139466983556)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7173150408917884)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.float64(0.7222091684137489),\n",
    "  np.float64(0.7216677879943186),\n",
    "  np.float64(0.7136651633809828),\n",
    "  np.float64(0.7179334171846805),\n",
    "  np.float64(0.7110996674852108)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
