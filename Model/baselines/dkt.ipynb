{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nogaschw/.conda/envs/clean_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "sys.path.append('../..')\n",
    "from Model.helper import *\n",
    "from Config import Config\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../Data'))\n",
    "from Data import *\n",
    "from choosedataset import *\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev_comp_cons\n",
      "30    11995\n",
      "Name: count, dtype: int64\n",
      "10 False similarity: False\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "data = [Codeworkout, Falcon][config.dataset]()\n",
    "df = data.df\n",
    "padding_size_code = 765\n",
    "loss_func = False\n",
    "df['num_snapshots'] = df['prev_tasks'].apply(lambda x: [len(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_future_q = set()\n",
    "for i in df['new_task_id']:\n",
    "    all_future_q.add(i)\n",
    "\n",
    "all_prev_q = set()\n",
    "for i in df['prev_tasks_id']:\n",
    "    all_prev_q = all_prev_q.union(set(i))\n",
    "all_problems = all_future_q.union(all_prev_q)\n",
    "vocab = {name: idx for idx, name in enumerate(all_problems)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1 - With last attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentDataset(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, max_len_code=768, padding_size_code=100, padding_size_q=30):\n",
    "        self.df = df\n",
    "        self.vocab = text_tokenizer\n",
    "        self.vocab['empty'] = len(text_tokenizer)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            'prev_problems_id': torch.tensor([self.vocab.get(pid) for pid in row[\"prev_tasks_id\"]]),\n",
    "            'prev_labels': torch.tensor(row['prev_labels']),\n",
    "            'future_problem_id': torch.tensor(self.vocab.get(row['new_task_id'])),\n",
    "            'label': torch.tensor(row['Label'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTWithFutureTaskID(nn.Module):\n",
    "    def __init__(self, num_tasks, embed_size=3, lstm_hidden_size=512):\n",
    "        super(DKTWithFutureTaskID, self).__init__()\n",
    "        self.task_embedding = nn.Embedding(num_tasks, embed_size)        # Embedding layer for the task IDs\n",
    "        self.lstm = nn.LSTM(input_size=embed_size + 1, hidden_size=lstm_hidden_size, num_layers=1, batch_first=True)  # Task embedding + binary success/failure\n",
    "        self.fc = nn.Linear((lstm_hidden_size + embed_size), 1)\n",
    "        self.fc_all = nn.Linear(lstm_hidden_size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, past_task_ids, past_successes, future_task_id):\n",
    "        past_task_embeddings = self.task_embedding(past_task_ids)  # Shape: (batch_size, num_past_tasks, embed_size)\n",
    "        past_input = torch.cat([past_task_embeddings, past_successes.unsqueeze(-1).float()], dim=-1)          \n",
    "        lstm_out, _ = self.lstm(past_input)  \n",
    "        final_lstm_out = lstm_out[:, -1, :] # Take the final LSTM output (from the last time step)        \n",
    "        future_task_embeddings = self.task_embedding(future_task_id) # Embed the future task ID \n",
    "        combined_input = torch.cat([final_lstm_out, future_task_embeddings], dim=-1) # Combine the LSTM output with the future task embedding\n",
    "        output = self.fc(combined_input)\n",
    "        all_question_preds = self.sig(self.fc_all(lstm_out))\n",
    "        return all_question_preds, self.sig(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2 - With all attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentDataset(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, max_len_code=768, padding_size_code=100, padding_size_q=30):\n",
    "        self.df = df\n",
    "        self.padding = padding_size_code\n",
    "        self.vocab = text_tokenizer\n",
    "        self.vocab['empty'] = len(text_tokenizer)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        prev_problem_id = torch.zeros((self.padding), dtype=torch.long)\n",
    "        prev_label = torch.zeros((self.padding), dtype=torch.long)\n",
    "        c = 0\n",
    "        for i, s in enumerate(row['num_snapshots']):\n",
    "            prev_problem_id_i = self.vocab.get(row[\"prev_tasks_id\"][i])\n",
    "            for j in range(s):\n",
    "                prev_problem_id[c] = prev_problem_id_i\n",
    "                prev_label[c] = row[\"prev_labels\"][i] if s - 1 == j else False\n",
    "                c += 1\n",
    "        return {\n",
    "            'code_num': torch.tensor(sum(row['num_snapshots'])), \n",
    "            'prev_problems_id': prev_problem_id,\n",
    "            'prev_labels': prev_label,\n",
    "            'future_problem_id': torch.tensor(self.vocab.get(row['new_task_id'])),\n",
    "            'label': torch.tensor(row['Label'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTWithFutureTaskID(nn.Module):\n",
    "    def __init__(self, num_tasks, embed_size=3, lstm_hidden_size=64):\n",
    "        super(DKTWithFutureTaskID, self).__init__()\n",
    "        self.task_embedding = nn.Embedding(num_tasks, embed_size)        # Embedding layer for the task IDs\n",
    "        self.lstm = nn.LSTM(input_size=embed_size + 1, hidden_size=lstm_hidden_size, num_layers=1, batch_first=True)  # Task embedding + binary success/failure\n",
    "        self.fc = nn.Linear((lstm_hidden_size + embed_size), 1)\n",
    "        self.fc_all = nn.Linear(lstm_hidden_size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, code_num, past_task_ids, past_successes, future_task_id):\n",
    "        past_task_embeddings = self.task_embedding(past_task_ids)  # Shape: (batch_size, num_past_tasks, embed_size)\n",
    "        past_input = torch.cat([past_task_embeddings, past_successes.unsqueeze(-1).float()], dim=-1)\n",
    "        snapshots_lstm = pack_padded_sequence(\n",
    "            past_input,\n",
    "            lengths=code_num.to('cpu'),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        packed_output, (hn, cn) = self.lstm(snapshots_lstm)  \n",
    "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)  # (batch_size, max_seq_length, lstm_hidden_size)\n",
    "        future_task_embeddings = self.task_embedding(future_task_id) # Embed the future task ID \n",
    "        combined_input = torch.cat([hn[-1], future_task_embeddings], dim=-1) # Combine the LSTM output with the future task embedding\n",
    "        output = self.fc(combined_input)\n",
    "        all_question_preds = self.sig(self.fc_all(lstm_out))\n",
    "        return all_question_preds, self.sig(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Option - caculate also all the past tasks loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc = True\n",
    "class lossFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lossFunc, self).__init__()\n",
    "        self.crossEntropy = nn.BCELoss()\n",
    "\n",
    "    def forward(self, all_pred, target_prev, code_num, target_label):\n",
    "        loss = 0\n",
    "        pred, target_q = all_pred\n",
    "        pred = pred.to('cpu')\n",
    "        code_num = code_num.to('cpu')\n",
    "        target_q = target_q.to('cpu')\n",
    "        target_prev = target_prev.to('cpu')\n",
    "        target_label = target_label.to('cpu').unsqueeze(1)\n",
    "        for batch in range(pred.shape[0]):\n",
    "            s = code_num[batch]\n",
    "            p = torch.cat([pred.squeeze(-1)[batch, :s], target_q[batch]])\n",
    "            a = torch.cat([target_prev.squeeze(-1)[batch, :s], target_label[batch]])\n",
    "            loss += self.crossEntropy(p, a)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_1loss(batch, model, device, criterion):\n",
    "    dict_batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    model_params = {k: v for k, v in dict_batch.items() if k != 'label'}\n",
    "    logits = model(*model_params.values())\n",
    "    label = dict_batch['label'].float()\n",
    "    if not criterion:\n",
    "        return logits[1], label\n",
    "    if loss_func == False:\n",
    "        return criterion(logits[1], label.unsqueeze(1)) \n",
    "    return criterion(logits, batch['prev_label'], batch['code_num'], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DKTWithFutureTaskID(len(vocab))\n",
    "caculate_func = caculate_1loss\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=1e-4)\n",
    "\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test, val, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing splitting\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = create_data_loader(df, StudentDataset, padding_size_code=padding_size_code, \n",
    "                                                                         text_tokenizer=vocab, batch_size=config.batch_size\n",
    "                                                                         , create_split=False, ids_filepath_prefix='/home/nogaschw/Codeworkout/UseData/both/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 80 162\n",
      "Label\n",
      "False    10898\n",
      "True      7071\n",
      "Name: count, dtype: int64\n",
      "Label\n",
      "False    1542\n",
      "True     1007\n",
      "Name: count, dtype: int64\n",
      "Label\n",
      "False    3145\n",
      "True     2035\n",
      "Name: count, dtype: int64\n",
      "931 132 267\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader), len(valid_dataloader), len(test_dataloader), flush=True)\n",
    "print(train_dataloader.dataset.df['Label'].value_counts())\n",
    "print(valid_dataloader.dataset.df['Label'].value_counts())\n",
    "print(test_dataloader.dataset.df['Label'].value_counts())\n",
    "print(len(set(train_dataloader.dataset.df['student_id'])), len(set(valid_dataloader.dataset.df['student_id'])), len(set(test_dataloader.dataset.df['student_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Batch 0 from 562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34708/3534076799.py:16: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'label': torch.tensor(row['Label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [1], LR: 0.000100, Loss: 0.6616, Val Loss: 0.6497, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 1\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [2], LR: 0.000100, Loss: 0.6511, Val Loss: 0.6537, patience: 5\n",
      "Epoch: 2\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [3], LR: 0.000100, Loss: 0.6510, Val Loss: 0.6502, patience: 4\n",
      "Epoch: 3\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [4], LR: 0.000100, Loss: 0.6499, Val Loss: 0.6510, patience: 3\n",
      "Epoch: 4\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [5], LR: 0.000100, Loss: 0.6493, Val Loss: 0.6487, patience: 2\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 5\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [6], LR: 0.000100, Loss: 0.6486, Val Loss: 0.6477, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 6\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [7], LR: 0.000100, Loss: 0.6486, Val Loss: 0.6493, patience: 5\n",
      "Epoch: 7\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [8], LR: 0.000100, Loss: 0.6481, Val Loss: 0.6611, patience: 4\n",
      "Epoch: 8\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [9], LR: 0.000100, Loss: 0.6481, Val Loss: 0.6478, patience: 3\n",
      "Epoch: 9\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [10], LR: 0.000100, Loss: 0.6468, Val Loss: 0.6466, patience: 2\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 10\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [11], LR: 0.000100, Loss: 0.6467, Val Loss: 0.6463, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 11\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [12], LR: 0.000100, Loss: 0.6463, Val Loss: 0.6466, patience: 5\n",
      "Epoch: 12\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [13], LR: 0.000100, Loss: 0.6458, Val Loss: 0.6462, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 13\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [14], LR: 0.000100, Loss: 0.6444, Val Loss: 0.6470, patience: 5\n",
      "Epoch: 14\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [15], LR: 0.000100, Loss: 0.6446, Val Loss: 0.6450, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 15\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [16], LR: 0.000100, Loss: 0.6444, Val Loss: 0.6437, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 16\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [17], LR: 0.000100, Loss: 0.6427, Val Loss: 0.6425, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 17\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [18], LR: 0.000100, Loss: 0.6419, Val Loss: 0.6458, patience: 5\n",
      "Epoch: 18\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [19], LR: 0.000100, Loss: 0.6423, Val Loss: 0.6424, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 19\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [20], LR: 0.000100, Loss: 0.6409, Val Loss: 0.6409, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 20\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [21], LR: 0.000100, Loss: 0.6400, Val Loss: 0.6432, patience: 5\n",
      "Epoch: 21\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [22], LR: 0.000100, Loss: 0.6393, Val Loss: 0.6435, patience: 4\n",
      "Epoch: 22\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [23], LR: 0.000100, Loss: 0.6379, Val Loss: 0.6416, patience: 3\n",
      "Epoch: 23\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [24], LR: 0.000100, Loss: 0.6377, Val Loss: 0.6381, patience: 2\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 24\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [25], LR: 0.000100, Loss: 0.6377, Val Loss: 0.6384, patience: 5\n",
      "Epoch: 25\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [26], LR: 0.000100, Loss: 0.6356, Val Loss: 0.6371, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 26\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [27], LR: 0.000100, Loss: 0.6347, Val Loss: 0.6366, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 27\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [28], LR: 0.000100, Loss: 0.6335, Val Loss: 0.6379, patience: 5\n",
      "Epoch: 28\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [29], LR: 0.000100, Loss: 0.6326, Val Loss: 0.6341, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 29\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [30], LR: 0.000100, Loss: 0.6322, Val Loss: 0.6349, patience: 5\n",
      "Epoch: 30\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [31], LR: 0.000100, Loss: 0.6313, Val Loss: 0.6333, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 31\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [32], LR: 0.000100, Loss: 0.6300, Val Loss: 0.6359, patience: 5\n",
      "Epoch: 32\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [33], LR: 0.000100, Loss: 0.6295, Val Loss: 0.6324, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 33\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [34], LR: 0.000100, Loss: 0.6281, Val Loss: 0.6309, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 34\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [35], LR: 0.000100, Loss: 0.6272, Val Loss: 0.6339, patience: 5\n",
      "Epoch: 35\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [36], LR: 0.000100, Loss: 0.6260, Val Loss: 0.6307, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 36\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [37], LR: 0.000100, Loss: 0.6257, Val Loss: 0.6300, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 37\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [38], LR: 0.000100, Loss: 0.6242, Val Loss: 0.6293, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 38\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [39], LR: 0.000100, Loss: 0.6232, Val Loss: 0.6263, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 39\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [40], LR: 0.000100, Loss: 0.6221, Val Loss: 0.6263, patience: 5\n",
      "Epoch: 40\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [41], LR: 0.000100, Loss: 0.6207, Val Loss: 0.6245, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 41\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [42], LR: 0.000100, Loss: 0.6201, Val Loss: 0.6265, patience: 5\n",
      "Epoch: 42\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [43], LR: 0.000100, Loss: 0.6191, Val Loss: 0.6236, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 43\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [44], LR: 0.000100, Loss: 0.6182, Val Loss: 0.6226, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 44\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [45], LR: 0.000100, Loss: 0.6170, Val Loss: 0.6238, patience: 5\n",
      "Epoch: 45\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [46], LR: 0.000100, Loss: 0.6164, Val Loss: 0.6214, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 46\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [47], LR: 0.000100, Loss: 0.6152, Val Loss: 0.6216, patience: 5\n",
      "Epoch: 47\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [48], LR: 0.000100, Loss: 0.6140, Val Loss: 0.6228, patience: 4\n",
      "Epoch: 48\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [49], LR: 0.000100, Loss: 0.6130, Val Loss: 0.6193, patience: 3\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 49\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [50], LR: 0.000100, Loss: 0.6122, Val Loss: 0.6176, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 50\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [51], LR: 0.000100, Loss: 0.6113, Val Loss: 0.6187, patience: 5\n",
      "Epoch: 51\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [52], LR: 0.000100, Loss: 0.6103, Val Loss: 0.6160, patience: 4\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 52\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [53], LR: 0.000100, Loss: 0.6091, Val Loss: 0.6168, patience: 5\n",
      "Epoch: 53\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [54], LR: 0.000100, Loss: 0.6081, Val Loss: 0.6174, patience: 4\n",
      "Epoch: 54\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [55], LR: 0.000100, Loss: 0.6074, Val Loss: 0.6151, patience: 3\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 55\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [56], LR: 0.000100, Loss: 0.6067, Val Loss: 0.6136, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 56\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [57], LR: 0.000100, Loss: 0.6057, Val Loss: 0.6119, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 57\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [58], LR: 0.000100, Loss: 0.6045, Val Loss: 0.6164, patience: 5\n",
      "Epoch: 58\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [59], LR: 0.000100, Loss: 0.6039, Val Loss: 0.6152, patience: 4\n",
      "Epoch: 59\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [60], LR: 0.000100, Loss: 0.6034, Val Loss: 0.6127, patience: 3\n",
      "Epoch: 60\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [61], LR: 0.000100, Loss: 0.6023, Val Loss: 0.6126, patience: 2\n",
      "Epoch: 61\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [62], LR: 0.000100, Loss: 0.6018, Val Loss: 0.6110, patience: 1\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 62\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [63], LR: 0.000100, Loss: 0.6006, Val Loss: 0.6128, patience: 5\n",
      "Epoch: 63\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [64], LR: 0.000100, Loss: 0.6002, Val Loss: 0.6112, patience: 4\n",
      "Epoch: 64\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [65], LR: 0.000100, Loss: 0.5995, Val Loss: 0.6097, patience: 3\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 65\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [66], LR: 0.000100, Loss: 0.5986, Val Loss: 0.6082, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 66\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [67], LR: 0.000100, Loss: 0.5978, Val Loss: 0.6082, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 67\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [68], LR: 0.000100, Loss: 0.5970, Val Loss: 0.6074, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 68\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [69], LR: 0.000100, Loss: 0.5963, Val Loss: 0.6053, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 69\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [70], LR: 0.000100, Loss: 0.5953, Val Loss: 0.6070, patience: 5\n",
      "Epoch: 70\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [71], LR: 0.000100, Loss: 0.5950, Val Loss: 0.6056, patience: 4\n",
      "Epoch: 71\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [72], LR: 0.000100, Loss: 0.5945, Val Loss: 0.6043, patience: 3\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 72\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [73], LR: 0.000100, Loss: 0.5938, Val Loss: 0.6077, patience: 5\n",
      "Epoch: 73\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [74], LR: 0.000100, Loss: 0.5928, Val Loss: 0.6065, patience: 4\n",
      "Epoch: 74\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [75], LR: 0.000100, Loss: 0.5924, Val Loss: 0.6060, patience: 3\n",
      "Epoch: 75\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [76], LR: 0.000100, Loss: 0.5918, Val Loss: 0.6054, patience: 2\n",
      "Epoch: 76\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [77], LR: 0.000100, Loss: 0.5912, Val Loss: 0.6032, patience: 1\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 77\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [78], LR: 0.000100, Loss: 0.5905, Val Loss: 0.6025, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 78\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [79], LR: 0.000100, Loss: 0.5898, Val Loss: 0.6027, patience: 5\n",
      "Epoch: 79\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [80], LR: 0.000100, Loss: 0.5892, Val Loss: 0.6027, patience: 4\n",
      "Epoch: 80\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [81], LR: 0.000100, Loss: 0.5888, Val Loss: 0.6028, patience: 3\n",
      "Epoch: 81\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [82], LR: 0.000100, Loss: 0.5885, Val Loss: 0.6025, patience: 2\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 82\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [83], LR: 0.000100, Loss: 0.5871, Val Loss: 0.6001, patience: 5\n",
      "success deep copy\n",
      "success save in a\n",
      "Epoch: 83\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [84], LR: 0.000100, Loss: 0.5873, Val Loss: 0.6011, patience: 5\n",
      "Epoch: 84\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [85], LR: 0.000100, Loss: 0.5864, Val Loss: 0.6023, patience: 4\n",
      "Epoch: 85\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [86], LR: 0.000100, Loss: 0.5857, Val Loss: 0.6009, patience: 3\n",
      "Epoch: 86\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [87], LR: 0.000100, Loss: 0.5852, Val Loss: 0.6005, patience: 2\n",
      "Epoch: 87\n",
      "Batch 0 from 562\n",
      "Batch 100 from 562\n",
      "Batch 200 from 562\n",
      "Batch 300 from 562\n",
      "Batch 400 from 562\n",
      "Batch 500 from 562\n",
      "Test Batch 0 from 80\n",
      "Epoch [88], LR: 0.000100, Loss: 0.5853, Val Loss: 0.6003, patience: 1\n",
      "28/07/2025_15:24:47\n",
      "Loaded best model weights.\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model = training_loop(model=model, train_dataloader=train_dataloader, test_dataloader=valid_dataloader, \n",
    "                      optimizer=optimizer, criterion=criterion, device=device, name='a', caculate_func=caculate_func, use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(threshold, y_true, y_prob):\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.where(y_prob > threshold, 1, 0)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    best = \"best\"\n",
    "    if threshold == 0.5:\n",
    "        best = \"0.5\"\n",
    "    #  df = pd.concat([pd.DataFrame([[model_name, threshold, roc_auc, accuracy, precision, recall, f1]], columns=df.columns), df], ignore_index=True)\n",
    "    print({\"threshold\": threshold, \"roc_auc\": roc_auc, \"accuracy\": accuracy, f\"precision_{best}\": precision, f\"recall_{best}\": recall, f\"f1_{best}\": f1})\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch 0 from 39\n",
      "Test Batch 0 from 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3437120/3534076799.py:16: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'label': torch.tensor(row['Label'])\n",
      "/tmp/ipykernel_3437120/3534076799.py:16: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'label': torch.tensor(row['Label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': np.float32(0.500011), 'roc_auc': np.float64(0.29765297750140346), 'accuracy': 0.7655201342281879, 'precision_best': np.float64(0.0), 'recall_best': np.float64(0.0), 'f1_best': np.float64(0.0)}\n",
      "[[1825    1]\n",
      " [ 558    0]]\n"
     ]
    }
   ],
   "source": [
    "all_labels, all_probs = eval_loop(model, valid_dataloader, device, caculate_func=caculate_func)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "J = tpr - fpr\n",
    "best_index = J.argmax()\n",
    "\n",
    "y_labels, y_probs = eval_loop(model, test_dataloader, device, caculate_func=caculate_func)\n",
    "results(thresholds[best_index], y_labels, y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = create_data_loader_k_fold(df, StudentDataset, vocab, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 76\n",
      "504 126\n",
      "set()\n",
      "Label\n",
      "False    0.765811\n",
      "True     0.234189\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.738085\n",
      "True     0.261915\n",
      "Name: proportion, dtype: float64\n",
      "300 76\n",
      "504 126\n",
      "set()\n",
      "Label\n",
      "False    0.757667\n",
      "True     0.242333\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.770444\n",
      "True     0.229556\n",
      "Name: proportion, dtype: float64\n",
      "302 74\n",
      "504 126\n",
      "set()\n",
      "Label\n",
      "False    0.770388\n",
      "True     0.229612\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.71871\n",
      "True     0.28129\n",
      "Name: proportion, dtype: float64\n",
      "299 77\n",
      "504 126\n",
      "set()\n",
      "Label\n",
      "False    0.752907\n",
      "True     0.247093\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.788807\n",
      "True     0.211193\n",
      "Name: proportion, dtype: float64\n",
      "301 74\n",
      "504 126\n",
      "set()\n",
      "Label\n",
      "False    0.754337\n",
      "True     0.245663\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "False    0.784206\n",
      "True     0.215794\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def num_of(train_dataloader, test_dataloader):\n",
    "    print(len(train_dataloader), len(test_dataloader))\n",
    "    print(len(set(train_dataloader.dataset.df['student_id'])), len(set(test_dataloader.dataset.df['student_id'])))\n",
    "    print(set(train_dataloader.dataset.df['student_id']).intersection(set(test_dataloader.dataset.df['student_id'])))\n",
    "    print(train_dataloader.dataset.df.Label.value_counts(normalize=True))\n",
    "    print(test_dataloader.dataset.df.Label.value_counts(normalize=True))\n",
    "\n",
    "for train, test in data_loaders:\n",
    "    num_of(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "DKTWithFutureTaskID(\n",
      "  (task_embedding): Embedding(51, 3)\n",
      "  (lstm): LSTM(4, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=515, out_features=1, bias=True)\n",
      "  (fc_all): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Batch 0 from 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43001/3534076799.py:16: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'label': torch.tensor(row['Label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Fold 1, Epoch 0: Loss = 0.5268443705638249\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Fold 1, Epoch 10: Loss = 0.46296677778164547\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Test Batch 0 from 76\n",
      "Fold 2:\n",
      "DKTWithFutureTaskID(\n",
      "  (task_embedding): Embedding(51, 3)\n",
      "  (lstm): LSTM(4, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=515, out_features=1, bias=True)\n",
      "  (fc_all): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Batch 0 from 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43001/3534076799.py:16: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'label': torch.tensor(row['Label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Fold 2, Epoch 0: Loss = 0.5361180188258489\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Fold 2, Epoch 10: Loss = 0.4687452530860901\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Batch 0 from 300\n",
      "Batch 100 from 300\n",
      "Batch 200 from 300\n",
      "Test Batch 0 from 76\n",
      "Fold 3:\n",
      "DKTWithFutureTaskID(\n",
      "  (task_embedding): Embedding(51, 3)\n",
      "  (lstm): LSTM(4, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=515, out_features=1, bias=True)\n",
      "  (fc_all): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Batch 0 from 302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43001/3534076799.py:16: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'label': torch.tensor(row['Label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Fold 3, Epoch 0: Loss = 0.5167976021766663\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Fold 3, Epoch 10: Loss = 0.4549291814301188\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Batch 0 from 302\n",
      "Batch 100 from 302\n",
      "Batch 200 from 302\n",
      "Batch 300 from 302\n",
      "Test Batch 0 from 74\n",
      "Fold 4:\n",
      "DKTWithFutureTaskID(\n",
      "  (task_embedding): Embedding(51, 3)\n",
      "  (lstm): LSTM(4, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=515, out_features=1, bias=True)\n",
      "  (fc_all): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Batch 0 from 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43001/3534076799.py:16: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'label': torch.tensor(row['Label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Fold 4, Epoch 0: Loss = 0.547784068413004\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Fold 4, Epoch 10: Loss = 0.4766977042657476\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Batch 0 from 299\n",
      "Batch 100 from 299\n",
      "Batch 200 from 299\n",
      "Test Batch 0 from 77\n",
      "Fold 5:\n",
      "DKTWithFutureTaskID(\n",
      "  (task_embedding): Embedding(51, 3)\n",
      "  (lstm): LSTM(4, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=515, out_features=1, bias=True)\n",
      "  (fc_all): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "Batch 0 from 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43001/3534076799.py:16: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'label': torch.tensor(row['Label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Fold 5, Epoch 0: Loss = 0.5379060093944651\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Fold 5, Epoch 10: Loss = 0.4712221094738209\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Batch 0 from 301\n",
      "Batch 100 from 301\n",
      "Batch 200 from 301\n",
      "Batch 300 from 301\n",
      "Test Batch 0 from 74\n"
     ]
    }
   ],
   "source": [
    "fold_results = {'ROC-AUC' : [], 'f1' : [], 'recall': [], \"precision\": [], 'calibration': [], 'f1-0.5': [], 'recall-0.5': [], 'precision-0.5': []}\n",
    "\n",
    "for fold, (train_dataloader, test_dataloader) in enumerate(data_loaders):\n",
    "    print(f\"Fold {fold + 1}:\")    # Prepare data for current fold\n",
    "    m = DKTWithFutureTaskID(len(vocab))\n",
    "    loss_fn = None\n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=config.lr, weight_decay=1e-4)\n",
    "\n",
    "    m = m.to(device)\n",
    "    print(m)\n",
    "    # Training Loop\n",
    "    for epoch in range(config.epoch):\n",
    "        total_loss = train_loop(m, train_dataloader, device, optimizer, criterion, caculate_func)\n",
    "\n",
    "        # Optional: Print metrics every few epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Fold {fold + 1}, Epoch {epoch}: Loss = {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "    y_labels, y_probs = eval_loop(m, test_dataloader, device, caculate_func=caculate_func)\n",
    "    y_prob = np.array(y_probs)\n",
    "    y_true = np.array(y_labels)\n",
    "    y_pred = np.where(y_prob > 0.25, 1, 0)\n",
    "\n",
    "    fold_results['ROC-AUC'].append(roc_auc_score(y_true, y_prob))\n",
    "    fold_results['calibration'].append(brier_score_loss(y_true, y_prob))\n",
    "    fold_results['precision'].append(precision_score(y_true, y_pred))\n",
    "    fold_results['recall'].append(recall_score(y_true, y_pred))\n",
    "    fold_results['f1'].append(f1_score(y_true, y_pred))\n",
    "\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    fold_results['precision-0.5'].append(precision_score(y_true, y_pred))\n",
    "    fold_results['recall-0.5'].append(recall_score(y_true, y_pred))\n",
    "    fold_results['f1-0.5'].append(f1_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codeworkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fold Results:\n",
      "ROC-AUC: 0.7631\n",
      "f1: 0.3863\n",
      "recall: 1.0000\n",
      "precision: 0.2399\n",
      "calibration: 0.2704\n",
      "f1-0.5: 0.3863\n",
      "recall-0.5: 1.0000\n",
      "precision-0.5: 0.2399\n"
     ]
    }
   ],
   "source": [
    "avg_results = {metric: np.mean(vals) for metric, vals in fold_results.items()}\n",
    "print(\"Average Fold Results:\")\n",
    "for metric, avg in avg_results.items():\n",
    "    print(f\"{metric}: {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROC-AUC': [np.float64(0.7757837653430373),\n",
       "  np.float64(0.7637406856020453),\n",
       "  np.float64(0.7349486520609516),\n",
       "  np.float64(0.7828296740446767),\n",
       "  np.float64(0.75824536555499)],\n",
       " 'f1': [0.4151067323481117,\n",
       "  0.37339635381498987,\n",
       "  0.4390728476821192,\n",
       "  0.3487352445193929,\n",
       "  0.3549843695727683],\n",
       " 'recall': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'precision': [0.2619146290924161,\n",
       "  0.22955583229555832,\n",
       "  0.28128977513788717,\n",
       "  0.21119281045751634,\n",
       "  0.21579391891891891],\n",
       " 'calibration': [np.float64(0.2628272252680712),\n",
       "  np.float64(0.27227323080963584),\n",
       "  np.float64(0.266314210236803),\n",
       "  np.float64(0.2727034817658093),\n",
       "  np.float64(0.2776495811516879)],\n",
       " 'f1-0.5': [0.4151067323481117,\n",
       "  0.37339635381498987,\n",
       "  0.4390728476821192,\n",
       "  0.3487352445193929,\n",
       "  0.3549843695727683],\n",
       " 'recall-0.5': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'precision-0.5': [0.2619146290924161,\n",
       "  0.22955583229555832,\n",
       "  0.28128977513788717,\n",
       "  0.21119281045751634,\n",
       "  0.21579391891891891]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fold Results:\n",
      "ROC-AUC: 0.6317\n",
      "f1: 0.5647\n",
      "recall: 1.0000\n",
      "precision: 0.3935\n",
      "calibration: 0.2774\n",
      "f1-0.5: 0.5647\n",
      "recall-0.5: 1.0000\n",
      "precision-0.5: 0.3935\n"
     ]
    }
   ],
   "source": [
    "avg_results = {metric: np.mean(vals) for metric, vals in fold_results.items()}\n",
    "print(\"Average Fold Results:\")\n",
    "for metric, avg in avg_results.items():\n",
    "    print(f\"{metric}: {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROC-AUC': [np.float64(0.6231794997801354),\n",
       "  np.float64(0.6257554945054946),\n",
       "  np.float64(0.6353311986403241),\n",
       "  np.float64(0.6295867529619225),\n",
       "  np.float64(0.644726720549987)],\n",
       " 'f1': [0.5802809143486642,\n",
       "  0.5688225538971807,\n",
       "  0.5583600613582484,\n",
       "  0.5632231981040011,\n",
       "  0.5527335342229875],\n",
       " 'recall': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'precision': [0.408729388942774,\n",
       "  0.39745075318655854,\n",
       "  0.3873089572451151,\n",
       "  0.392004657481079,\n",
       "  0.3819155264723379],\n",
       " 'calibration': [np.float64(0.27557462492198304),\n",
       "  np.float64(0.275163208611968),\n",
       "  np.float64(0.27928190554245735),\n",
       "  np.float64(0.2786528084347616),\n",
       "  np.float64(0.27813735768762793)],\n",
       " 'f1-0.5': [0.5802809143486642,\n",
       "  0.5688225538971807,\n",
       "  0.5583600613582484,\n",
       "  0.5632231981040011,\n",
       "  0.5527335342229875],\n",
       " 'recall-0.5': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'precision-0.5': [0.408729388942774,\n",
       "  0.39745075318655854,\n",
       "  0.3873089572451151,\n",
       "  0.392004657481079,\n",
       "  0.3819155264723379]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
